{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDPG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f5f6f042d43b51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff9d7dc7ed45a9b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CriticNet(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(env.observation_space.shape[0] + env.action_space.shape[0], 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        # self.fc2 = torch.nn.Linear(128, 128)\n",
    "        # self.relu2 = torch.nn.ReLU()\n",
    "        # self.fc3 = torch.nn.Linear(128, 128)\n",
    "        # self.relu3 = torch.nn.ReLU()\n",
    "        self.fc4 = torch.nn.Linear(128, env.action_space.shape[0])\n",
    "\n",
    "    def forward(self, observation, action):\n",
    "        x = torch.cat([observation, action], dim=1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        # x = self.relu2(self.fc2(x))\n",
    "        # x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "137104b826319789"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ActorNet(torch.nn.Module):\n",
    "    def __init__(self, env, boundary=[-2, 2]):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "\n",
    "        # self.fc2 = torch.nn.Linear(128, 128)\n",
    "        # self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        # self.fc3 = torch.nn.Linear(128, 128)\n",
    "        # self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        self.fc4 = torch.nn.Linear(128, env.action_space.shape[0])\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "        self.low = boundary[0]\n",
    "        self.high = boundary[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        # x = self.relu2(self.fc2(x))\n",
    "        # x = self.relu3(self.fc3(x))\n",
    "        x = self.tanh(self.fc4(x))\n",
    "        x = (self.high - self.low) / (1 - (-1)) * (x - (-1)) + self.low\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "510c3a5c5c3d006f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, env, batch_size=64):\n",
    "        self.critic = CriticNet(env)\n",
    "        self.critic_target = CriticNet(env)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=2e-3)\n",
    "        self.actor = ActorNet(env, boundary=[-2, 2])\n",
    "        self.actor_target = ActorNet(env, boundary=[-2, 2])\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.buffer = deque(maxlen=50000)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def choose_action(self, state, explore=True):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).item()\n",
    "        if not explore:\n",
    "            return [action]\n",
    "        else:\n",
    "            action = np.clip(np.random.normal(action, 1), -2, 2)\n",
    "            return [action.item()]\n",
    "\n",
    "    # def store_transition(self, state, action, reward, new_state):\n",
    "    #     _temp = [state, action, [reward], new_state]\n",
    "    #     self.buffer.append(_temp)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    def ema_update_target(self, source_model, target_model, tau=0.01):\n",
    "        weighted_sum_state_dict = {}\n",
    "        for key in source_model.state_dict():\n",
    "            weighted_sum_state_dict[key] = tau * source_model.state_dict()[key] + (1 - tau) * target_model.state_dict()[\n",
    "                key]\n",
    "        target_model.load_state_dict(weighted_sum_state_dict)\n",
    "\n",
    "    def learn(self):\n",
    "        # self.update_target()\n",
    "        self.ema_update_target(self.critic, self.critic_target)\n",
    "        self.ema_update_target(self.actor, self.actor_target)\n",
    "\n",
    "        batch_samples = random.sample(self.buffer, self.batch_size)\n",
    "        state_lst, action_lst, reward_lst, new_state_lst = zip(*batch_samples)\n",
    "        state_lst = torch.FloatTensor(state_lst)\n",
    "        action_lst = torch.FloatTensor(action_lst)\n",
    "        reward_lst = torch.FloatTensor(reward_lst)\n",
    "        new_state_lst = torch.FloatTensor(new_state_lst)\n",
    "        # print(state_lst.size())\n",
    "        # print(action_lst.size())\n",
    "        # print(reward_lst.size())\n",
    "        # print(new_state_lst.size())\n",
    "\n",
    "        # 更新critic网络\n",
    "        with torch.no_grad():\n",
    "            action_target = self.actor_target(new_state_lst)\n",
    "            new_q = self.critic_target(new_state_lst, action_target)\n",
    "        q_target = reward_lst + self.gamma * new_q\n",
    "        q_value = self.critic(state_lst, action_lst)\n",
    "        td_error = self.mse(q_target, q_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # 更新actor网络\n",
    "        action = self.actor(state_lst)\n",
    "        q_value = self.critic(state_lst, action)\n",
    "        loss_actor = -torch.mean(q_value)  # 寻找最小的loos_actor, 就是寻找最大的torch.mean(q_value), 就是使其q值最大\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def model_save(self, path):\n",
    "        torch.save({\n",
    "            'actor_model_state_dict': self.actor.state_dict(),\n",
    "            'actor_target_model_state_dict': self.actor_target.state_dict(),\n",
    "            'critic_model_state_dict': self.critic.state_dict(),\n",
    "            'critic_target_model_state_dict': self.critic_target.state_dict(),\n",
    "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
    "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def model_load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.actor.load_state_dict(checkpoint['actor_model_state_dict'])\n",
    "        self.actor_target.load_state_dict(checkpoint['actor_target_model_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['critic_model_state_dict'])\n",
    "        self.critic_target.load_state_dict(checkpoint['critic_target_model_state_dict'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d3e0d34c8f77f2d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log_dir = './runs'\n",
    "if os.path.exists(log_dir):\n",
    "    try:\n",
    "        shutil.rmtree(log_dir)\n",
    "        print(f'文件夹 {log_dir} 已成功删除。')\n",
    "    except OSError as error:\n",
    "        print(f'删除文件夹 {log_dir} 失败: {error}')\n",
    "else:\n",
    "    os.makedirs(log_dir)\n",
    "    print(f'文件夹 {log_dir} 不存在，已创建文件夹 {log_dir}。')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcea42ccf99ae228",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(log_dir=log_dir)\n",
    "env = gym.make('Pendulum-v1')\n",
    "batch_size = 64\n",
    "ddpg = DDPG(env, batch_size)\n",
    "episode = 2000\n",
    "steps = 64 * 8\n",
    "all_reward = []\n",
    "for epoch in range(episode):\n",
    "    start_time = time.time()\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    episode_rewards = 0\n",
    "    while step < steps:\n",
    "        # choose action\n",
    "        action = ddpg.choose_action(state)\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "        ddpg.buffer.append([state, action, [-(reward) ** 2 / 10], new_state])\n",
    "        state = new_state\n",
    "        episode_rewards += reward\n",
    "        step += 1\n",
    "        if step % batch_size == 0 or step == steps:\n",
    "            ddpg.learn()\n",
    "    all_reward.append(episode_rewards)\n",
    "    summary_writer.add_scalar('episode_rewards', episode_rewards, epoch)\n",
    "    print(\"Epoch/Episode: {}/{},reward: {}\".format(epoch + 1, episode, episode_rewards))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbcb91b0b5622bc0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "state, _ = env.reset()\n",
    "step = 0\n",
    "episode_rewards = 0\n",
    "while True:\n",
    "    a = ddpg.choose_action(torch.tensor(state), False)\n",
    "    new_state, reward, done, _, _ = env.step(a)\n",
    "    step += 1\n",
    "    state = new_state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8505b2d8e772f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4467a7db8e39694"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ddpg.model_save('DDPG Pendulum-v1.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd2ccdd3505d81b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
